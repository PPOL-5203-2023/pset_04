{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40babd62",
   "metadata": {},
   "source": [
    "<h1><center> PPOL 5203 Data Science I: Foundations <br><br> \n",
    "<font color='grey'>Problem Set III<br><br>\n",
    "Tiago Ventura</center></center> <h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2878ca7",
   "metadata": {},
   "source": [
    "# Introduction to Problem Set 04\n",
    "\n",
    "This problem set will focus on acquiring data on the web. Some of the assignments below are focused on keeping you up-to-speed with the tools we saw in class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea9a3e",
   "metadata": {},
   "source": [
    "## IMPORTANT: Remember to comment your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc8ecb",
   "metadata": {},
   "source": [
    "## 1. Scrapping the American Congress (20pts) (THIS QUESTION CAN ONLY BE SOLVED WITH SELENIUM)\n",
    "\n",
    "For your first scraping exercise, you will learn how to collect some data from the American Congress Website. Granted, some of this information might be available elsewhere, however, this is for you to practice, and the structure of their website is a easy one.\n",
    "\n",
    "This is the primary url for you to work on: https://www.congress.gov/search\n",
    "\n",
    "**Task:** Collect information about the **members** of the 118th congress available in the website. You **should** collect: \n",
    "\n",
    "- the url for each individual page\n",
    "\n",
    "- Name of the member\n",
    "\n",
    "- Party of the member\n",
    "\n",
    "- State of the member\n",
    "\n",
    "- District of the member\n",
    "\n",
    "- Where they serve (house or senate)\n",
    "\n",
    "- Their personal website if available\n",
    "\n",
    "- Convert all to a dataframe\n",
    "\n",
    "**Hint**\n",
    "\n",
    "- *The first step is to identify our typical page. Go to www.congress.gov > Advanced Searches  and refine the query to contain only Members from your desired legislature. Additionally, ask the query to show 250 results per page. From this, try to learn how pagination works*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e75ee",
   "metadata": {},
   "source": [
    "## 2. Building a scraper (20pts) (THIS QUESTION CAN ONLY BE SOLVED WITH SELENIUM)\n",
    "\n",
    "Now, generalize you previous scrapper. Collect the same information for other three legislatures other than 118<sup>th</sup> Congress. Each of the three should be on a different decade. To do so: \n",
    "\n",
    "- write one or multiple functions generalizing your scrapper\n",
    "\n",
    "- apply your function(s) to the other three legislature\n",
    "\n",
    "- save all as a csv. \n",
    "\n",
    "**Tip**\n",
    "\n",
    "- *Remember to work with try and except to deal with errors when you are collecting data that varies over time!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df182d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here. Remember to break in multiple chuncks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3fd14",
   "metadata": {},
   "source": [
    "## Replacement for Questions 1 and 2\n",
    "\n",
    "Some of you may have noticed, but the congress.gov website is actually blocking our scrappers. I apologize for this issue. When designing the homework, I used a previous scraper for question 1 and 2, and it was working properly. This is probably a reaction from Congress against the use of scraping to train LLM models. For this reason, you problem set 4 should be slightly modified.\n",
    "\n",
    "If you don't want to use selenium and want to practice with BeautifulSoup, do the following: \n",
    "\n",
    "Instead of solving question 1, you should solve an additional question, as described below:\n",
    "\n",
    "- Scrape the wikipedia webpage for current members of the congress. This is the url: https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives\n",
    "- Collect information only from the table: Voting members by state.\n",
    "- You should collect: District, Member, Party, Prior Experience, Education, Assumed Office, Residence, Born.\n",
    "- Convert this to a dataframe, and save.\n",
    "\n",
    "Instead of solving question 2, you should do the following:\n",
    "\n",
    "- From the same table as below, collect all the wikipedia pages available of the members of the congress\n",
    "\n",
    "\n",
    "Read this post here, it will help you on scraping wikipedia: https://scrapfly.io/blog/how-to-scrape-tables-with-beautifulsoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here. Remember to break in multiple chuncks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adb630",
   "metadata": {},
   "source": [
    "## 3. Accessing the NYT API (30pts)\n",
    "\n",
    "For this exercise, we will work with the New York Times API. The idea here is for you to get practice interacting with an API. Your task is to build a simple wrapper for you to interact with the API, search for articles, and save your searches. \n",
    "\n",
    "Let's get started. Your tasks are: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d72a0",
   "metadata": {},
   "source": [
    "#### 3.1 - Get access to the New York York Times API\n",
    "\n",
    "- Follow the instructions here to get your credentials: https://developer.nytimes.com/get-started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print here just the first half of your api-key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53750e73",
   "metadata": {},
   "source": [
    "#### 3.2 Search API\n",
    "\n",
    "When you sign up for the account, you must register an application and choose which APIs you want to activate. Some options are Article Search API, Top Stories API, and Archive API but also include Movies Review or Book API. \n",
    "\n",
    "We will be working with the Search API. \n",
    "\n",
    "Read more about the search API here: https://developer.nytimes.com/docs/articlesearch-product/1/overview\n",
    "\n",
    "In your words, describe to me the main components of the search api. These are:\n",
    "\n",
    "- endpoint:\n",
    "\n",
    "- describe some of the main filters\n",
    "\n",
    "- Are there any require filters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3882d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a41f9",
   "metadata": {},
   "source": [
    "#### 3.3 - Make your first API call\n",
    "\n",
    "Write code to make a simple API call. \n",
    "\n",
    "- Query the API to search for articles about Biden. \n",
    "\n",
    "- Make a get request\n",
    "\n",
    "- print only the status of the response\n",
    "\n",
    "**Tip**:\n",
    "\n",
    "- *the api key here is passed as a parameter, not as a http header*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a32f56",
   "metadata": {},
   "source": [
    "#### 3.4 - Add more elements to your query\n",
    "\n",
    "Now, add more parameters to your query. You should add the following parameters: \n",
    "    \n",
    "- begin_date\n",
    "\n",
    "- end_date\n",
    "\n",
    "- filters (only fq parameter)\n",
    "\n",
    "- pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81128249",
   "metadata": {},
   "source": [
    "#### 3.5 - Build a function for your wrapper\n",
    "\n",
    "Now, build a function to your wrapper. The inputs of the function should be all the parameters listed above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070bdd6a",
   "metadata": {},
   "source": [
    "#### 3.6 Rate-Limits\n",
    "\n",
    "One thing to note about the Times API is that it has a limit of 10 queries per minute or 4000 per day. Also, as you noticed before, a single request to the API only gives you up to 10 articles. \n",
    "\n",
    "In order to circumvent that but still get the data enough data, you need to work with the pagination of the API. \n",
    "\n",
    "Collect at least 100 results for any given query. To do so, you need to: \n",
    "\n",
    "- write a loop\n",
    "\n",
    "- add a pause inside of your loop so that you will do at most 10 queries per minute. \n",
    "\n",
    "- collect all the data and store in a single object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e33329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7654892",
   "metadata": {},
   "source": [
    "## 3.7 - Cleaning your data\n",
    "\n",
    "Now that you have collected at least one hundred articles from the NYT API, write a function to clean the data and save as a pandas dataframe. Your dataframe should have the following columns:\n",
    "\n",
    "- abstract\n",
    "\n",
    "- url of the articles\n",
    "\n",
    "- lead paragraph of the article\n",
    "\n",
    "- source\n",
    "\n",
    "- keywords\n",
    "\n",
    "- headline\n",
    "\n",
    "- Authors\n",
    "\n",
    "**Hint**\n",
    "\n",
    "- The data from the API comes as a string. To convert to data type you can process, you need to convert to a json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "705fdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc8400",
   "metadata": {},
   "source": [
    "## 4. Working with Selenium (30pts)\n",
    "\n",
    "In class, I show you an code I wrote to collect data from a News Aggregator in China. The code is provided in the repository as a .py file. This question will test your skills on understanding the code provided to you. Here are the tasks: \n",
    "\n",
    "1. Install selenium and sucessfully instantiate a object from the class `ToutiaoBot()`\n",
    "\n",
    "2. Collect all article recommendations from the home feed of your bot. \n",
    "\n",
    "3. From five of these article, collect all their recommendations. \n",
    "\n",
    "4. Combine all the url (from the home feed and reccomended articles), and collect the metadata for all articles. \n",
    "\n",
    "5. Save all as a pandas dataframe.\n",
    "\n",
    "**Hint**: You don't need to write any scrapper. All the tasks above can be accomplished from the code I provided to you. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here. Remember to break in multiple chuncks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
